\chapter{Comparative Analysis}
\label{ch:comparative}

This chapter contrasts reinforcement learning schedulers against established heuristics and mathematical programming approaches across the manufacturing domains covered by the PRISMA review. For each scenario we highlight the modeling choices, baseline gaps, and evidence quality to inform practitioners considering deployment.

\section{Job-Shop and Flexible Manufacturing Cells}
Deep RL for job shops typically learns dispatching policies that subsume classical rules such as Shortest Processing Time (SPT) or Apparent Tardiness Cost (ATC). Convolutional encoders over machine queues and remaining processing times deliver 5--10\,\% makespan gains on Taillard benchmarks relative to tabu search, while maintaining sub-second inference once trained \cite{Zhang2020DeepRLJSSP}. Graph neural network (GNN) policies extend this idea by embedding precedence graphs, which improves generalization when job counts change between training and deployment \cite{Liu2021GraphRLJSSP}. Hybrid flow shops and flexible lines reuse these representations but add stage-level agents that handle buffer coordination, producing robust throughput under fluctuating product mixes \cite{Park2021MARLFJSP,Santos2021HierHFS}.

Reward shaping and curriculum design remain decisive levers. PPO policies with calendar-aware states adapt to rush orders provided the training regime gradually increases shop complexity \cite{Chen2022AdaptiveFJSP}. Hybrid RL+CP pipelines use a policy network to propose machine assignments before a CP-SAT refiner enforces hard constraints, cutting solve time against pure mathematical models without sacrificing feasibility \cite{Kumar2023HybridRL}. Continual learning and human-in-the-loop feedback mitigate catastrophic forgetting when routing rules evolve; operators can mark undesirable decisions, and the policy integrates the feedback through preference gradients \cite{Zhou2024ContinualFJSP}. Meta-RL further shortens adaptation windows: aerospace flexible cells fine-tune a meta-policy within a handful of gradient steps when new fixtures appear, recovering near-optimal takt times compared with line engineers’ handcrafted rules \cite{Meier2025TransferAeroRL}.

Resilience to disruptions is increasingly modeled explicitly. Decentralized MARL assigns agents to machines, rewarding both makespan reduction and rapid recovery after breakdowns, outperforming robust tabu search under the same stochastic disturbance patterns \cite{Nguyen2022RobustMARL,RL-MARL-ROBUST-2022}. Collaborative robot islands add safety envelopes and shared fixture coordination to the state space; actor-critic dispatchers respect collision zones while shortening changeovers relative to manual sequencing \cite{Kawasaki2023RobotCell}. Together these studies show that RL schedulers can meet or exceed long-standing heuristics while offering knobs for adaptation, provided sufficient digital twin fidelity exists.

\section{Flow-Shop, Hybrid, and Aerospace Lines}
Flow-shop RL research commonly adopts hierarchical or multi-agent designs in which stage-level policies select the next job and lower-level controllers decide machine-specific actions. Early hierarchical formulations demonstrated that manager-worker structures reduce buffer starvation and outperform NEH or genetic algorithms on classic benchmarks \cite{Santos2021HierHFS}. Subsequent multi-objective variants introduce reward scalarization or Pareto-search hybrids; actor-critic proposals warm-start NSGA-II fronts, yielding schedules that simultaneously improve makespan, energy, and tardiness compared with evolutionary solvers alone \cite{Garcia2023MORL,Rao2024HybridMOHRL}.

Digital twin connectivity is now a differentiator. Battery production lines stream sensor traces into a graph RL controller that retrains nightly, closing the sim-to-real gap and holding throughput within 1\,\% of offline optima amid mix changes \cite{Huang2024BatteryGraphRL}. Real-time twin feedback can even keep the policy synchronized with layout changes; Lopez et~al.\ inject telemetry into the RL pipeline every few seconds, avoiding performance drift when buffers are temporarily repurposed for rework \cite{Lopez2025StreamingTwinRL}. Aerospace assembly introduces human collaboration constraints: hierarchical actor-critic schedulers maintain takt-time adherence while respecting ergonomic windows and manual fastening precedences, cutting schedule violations versus heuristic takt balancing \cite{Sanchez2024AerospaceHRL,RL-MANUF-2024}. The combination of RL inference speed, twin-based situational awareness, and OR refinements therefore provides a compelling toolkit for complex flow lines.

\section{Semiconductor and High-Mix Electronics}
Semiconductor fabs exemplify high-dimensional scheduling with re-entrant flows, batching, and energy tariffs. Double DQN dispatchers configured on 300\,mm fab models reduce cycle time and work-in-process (WIP) by 6--9\,\% compared with rule-based schedulers while honoring reticle availability constraints \cite{Lee2020WaferRL}. Cluster tools benefit from decentralized actor-critic agents that coordinate wafer transfers and alleviate transport congestion, outperforming petri-net baselines on throughput and utilization \cite{Chen2022WaferMARL}. Policy-gradient EUV schedulers incorporate mask-cleaning windows and stochastic tool failures, reducing lot-to-lot variability compared with handcrafted heuristics \cite{Kim2023EUV}. Recent work harnesses GNN embeddings to share context across lithography clusters, achieving better cold-start performance on unseen product mixes \cite{Park2024ClusterGraphRL}.

Transfer and self-play techniques tackle data scarcity. Inter-fab transfer learning shares latent representations across plants with different toolsets, slashing the number of simulated episodes required for convergence by half \cite{He2023TransferFab}. Self-play EUV schedulers iteratively compete against prior versions to explore adversarial lot releases, closing the gap with MILP solutions on small cases while scaling to production-sized horizons \cite{Feng2025SelfPlayEUV}. Multi-objective formulations penalize both cycle time and energy draw, yielding schedules that dominate weighted-sum heuristics on Pareto fronts \cite{Morales2024MultiObjSemiconductorRL,Li2022EnergyFabSAC}. Despite these gains, reproducibility is limited: most studies rely on proprietary fabs or anonymized digital twins, so sharing sanitized simulators remains a priority for replication.

\section{Energy, Microgrid, and Sustainability-Oriented Scheduling}
Energy-aware RL schedulers introduce additional decision variables such as on/off states, speed settings, and tariff-sensitive release times. Early dueling-DQN methods demonstrated 8\,\% energy savings while capping makespan degradation at 2\,\% relative to deterministic shedding policies \cite{Gao2021EnergyRL}. PPO-LSTM controllers extend this idea by modeling tariff time series and dynamically pausing low-priority jobs, reducing both tardiness and demand peaks compared with rule-based demand-response programs \cite{Wang2023PPOEnergy}. Multi-agent SAC systems coordinate production and onsite storage assets in microgrid-integrated factories, explicitly exchanging carbon prices to arbitrate when to curtail loads versus draw from batteries \cite{Almeida2022MicrogridMARL,Ghosh2024CarbonMicrogridRL}. Semiconductor-specific studies fold cleanroom HVAC costs into the reward, ensuring that cycle-time gains do not come at the expense of excessive energy spikes \cite{Li2022EnergyFabSAC}.

Sustainability objectives increasingly extend beyond electricity. Battery EV module lines evaluate RL schedulers on throughput, energy, and quality metrics simultaneously, showing a 15\,\% reduction in scrap versus greedy heuristics when quality penalties enter the reward \cite{Muller2023EVModuleRL}. Real-time digital twins feed equipment health and energy telemetry back into the agent, enabling anticipatory maintenance actions that would otherwise require offline rescheduling \cite{Lopez2025StreamingTwinRL}. Collectively, these studies suggest that RL can encode enterprise-level sustainability KPIs provided high-fidelity simulations exist to expose long-horizon effects during training.

\section{Specialized Cells: Robotics, Pharma, and Human-Centric Lines}
Collaborative robot cells place additional emphasis on safety envelopes, shared fixtures, and operator coexistence. Actor-critic dispatchers tailored to robot-human stations enforce spatial constraints while still trimming changeover time by 7\,\% relative to manual coordination, mainly by predicting when to pre-stage fixtures before human arrivals \cite{Kawasaki2023RobotCell}. Aerospace and medical device cells exploit transfer learning to re-use policies across similar setups, so onboarding a new tool requires minutes rather than days of retuning \cite{Meier2025TransferAeroRL}. Pharmaceutical production introduces batching and patient-specific windows; RL schedulers penalize partial fills and simultaneously respect stability horizons, beating MILP and heuristic batching policies on both service levels and equipment utilization \cite{Patel2021BatchRL,Singh2024ContinuousPharmaRL}.

These specialized domains underscore two requirements for trustworthy RL deployment. First, reward design must cover regulatory or safety constraints that would invalidate a schedule regardless of its efficiency. Second, interpretable summaries (e.g., attention maps showing why a job was prioritized) are necessary for human supervisors to accept automated dispatching. By embedding these guardrails, RL schedulers become viable co-pilots even in highly regulated manufacturing niches.

\section{Interpretability and Human-in-the-Loop Governance}
Across domains, the comparative evidence highlights the growing emphasis on interpretability. Flexible job-shop studies now report saliency analyses on queue embeddings to show which machines influenced a decision \cite{RL-FLEX-2024B}. Semiconductor works expose surrogate models that approximate RL actions with decision trees to satisfy fab certification requirements \cite{RL-SEMICON-CHAIN-2025}. Energy-aware factories provide operator dashboards indicating how carbon budgets shaped throttling actions \cite{RL-ENERGY-2025}. Human feedback loops—either preference learning or override mechanisms—appear in several studies \cite{RL-FLEX-2025,RL-ROBOTCELL-2025}, demonstrating that RL need not be purely autonomous. Instead, policies can negotiate with planners: the agent proposes a schedule, humans approve or tweak steps, and the policy incorporates the feedback during fine-tuning. This governance model may prove decisive for scaling RL beyond pilot projects.
