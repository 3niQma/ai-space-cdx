paper_id,title,year,domain,problem_class,scheduler_environment,algorithm_family,state_design,action_design,reward_design,constraints,baselines,metrics,dataset_or_instance,code_available,generalization_tests,key_findings,notes
wang2021_csms,A Review of Reinforcement Learning Based Intelligent Optimization for Manufacturing Scheduling,2021,Manufacturing scheduling,Various (job shop/flow shop variants),Simulation studies summarized,Review (RL for scheduling),Summarizes common encodings (job/machine-centric),Surveys dispatching/RL policy actions,Discusses composite/tardiness/energy rewards,Constraint handling via masking/penalties (survey),Classical heuristics/metaheuristics (surveyed),Makespan,tardiness,energy (reported),Multiple benchmark instances (Taillard/FT/etc.) discussed,No,Not assessed,Provides taxonomy of state/action/reward designs and fusion with metaheuristics,Review article primary focus
panzer2022_ijpr,Deep Reinforcement Learning in Production Systems: A Systematic Literature Review,2022,Production systems scheduling/dispatching,Production planning/control (JSS/FJSS/dynamic),Simulation focus,Review (Deep RL applications),Surveys feature designs incl. graph/sequence encodings,Action spaces for dispatching/routing/loading,Reward shaping for throughput/tardiness,Highlights constraint-awareness needs,Presents comparisons to heuristics and OR baselines,Throughput,makespan,tardiness,Ongoing simulations on shop-floor benchmarks,No,Limited (notes need for robustness tests),Deep RL competitive vs heuristics; calls out safety/reliability gap,Systematic review scope
panzer2021_cpsl,Deep Reinforcement Learning in Production Planning and Control: A Systematic Literature Review,2021,Production planning/control,Dynamic production/logistics settings,Simulation focus,Review (Deep RL),Discusses sensor-driven state reps,Dispatching/transport decisions,Rewards aligning with delivery/throughput KPIs,Notes feasibility concerns (masking vs penalties),Heuristics and rule-based baselines,Throughput,service level,makespan,Simulated production and logistics cases,No,Not assessed,DL-based policies outperform heuristics in sim; reliability for real-world open,Conference SLR
modrak2024_alg,A Review on Reinforcement Learning in Production Scheduling: An Inferential Perspective,2024,Production scheduling,Broad (1996â€“2024 RL scheduling),Bibliometric/SLR,Summarizes state features across corpus,Policy actions for machine/job assignment,Broad reward forms (makespan/tardiness),Mentions constraint handling generically,Reviews heuristics/metaheuristics baselines,Makespan,tardiness (bibliometric focus),Scopus/ScienceDirect corpus,No,Not assessed,Bibliometric trends show rising RL scheduling interest,Bibliometric review
waubert2022_jim,On Reliability of Reinforcement Learning Based Production Scheduling Systems: A Comparative Survey,2022,Production scheduling reliability,Job/flow shop reliability focus,Simulation and survey,Survey on reliability of DRL scheduling,State abstractions vs robustness,Action feasibility and overrides,Rewards incorporating reliability/robustness,Constraint preservation emphasized,Discusses heuristics and OR baselines,Robustness,reliability metrics (uptime, tardiness),Various PS scenarios (surveyed),No,Calls for robustness and safety testing,Identifies reliability definitions and gaps for DRL scheduling,Survey with reliability lens
panzer2022_ijpr_additional,Deep Reinforcement Learning in Production Systems: A Systematic Literature Review (details),2022,Production systems scheduling/dispatching,Production planning/control (dynamic JSS/FJSS),Simulation,Deep RL survey,Shop-floor sensors and order queues; sometimes graph embeddings,Dispatching/transport/loading decisions,Throughput/tardiness rewards with shaping,Notes feasibility issues; mentions masking vs penalties,Heuristics (EDD, SPT), simulation-based RL baselines,Throughput,makespan,tardiness,Simulated shop systems (various),No,Limited; calls for robustness tests,DL outperforms heuristics in sim; reliability and safety open,Expanded fields
wang2021_csms_additional,A Review of Reinforcement Learning Based Intelligent Optimization for Manufacturing Scheduling (details),2021,Manufacturing scheduling,Job/shop-flow variants,Simulation review,RL and metaheuristic fusion survey,Job/machine-centric features,Dispatching/machine assignment actions,Composite objectives (profit/efficiency/energy),Penalty-based and heuristic constraint handling,Heuristics/metaheuristics compared,Profit,efficiency,energy,tardiness,Various benchmark instances,No,Not assessed,Highlights RL+metaheuristics fusion potential,Expanded fields
chen2021_rldsps,Reinforcement Learning for Dynamic Shop Scheduling: A Review,2021,Dynamic job shop scheduling,Dynamic JSS,Simulation survey,Review (RL for dynamic JSS),Survey of queue/graph-based states,Dispatching/machine assignment,Rewards for tardiness/makespan reduction,Constraint handling via penalties/masking,Compares to dispatching heuristics, makespan/tardiness,Typical JSS benchmarks,No,Notes lack of generalization tests,Survey focus
zhang2020_gnnjss,Graph Reinforcement Learning for Job-Shop Scheduling,2020,Manufacturing scheduling,Job shop (static/dynamic),Simulation,Policy gradient with GNN encoder,Graph features (jobs, machines, operations),Dispatching action (select next operation),Weighted tardiness/makespan rewards,Feasibility via masking,Heuristics (EDD, SPT), OR baselines,Makespan/tardiness,FT10/20, Taillard instances,No,Limited (train/test split),GNN-based policy improves over heuristics
park2021_drlfjsp,Deep Reinforcement Learning for Flexible Job Shop Scheduling,2021,Manufacturing scheduling,Flexible job shop,Simulation,DQN/PPO variants,Queue/machine states,Machine routing + operation sequencing,Weighted tardiness/makespan,Penalty terms for infeasible assignments,Heuristics (EDD, SPT), metaheuristics, Makespan/tardiness,Classic FJSP instances,No,Limited robustness discussed,DRL competitive on FJSP benchmarks
gao2022_dynamicfjsp,Dynamic Flexible Job Shop Scheduling via Deep RL,2022,Manufacturing scheduling,Dynamic flexible job shop,Simulation,PPO/SAC,State includes dynamic arrivals and machine status,Dispatching/routing decisions,Weighted tardiness and completion rewards,Masking for machine availability,Heuristics and metaheuristics, Makespan/tardiness,Dynamic FJSP datasets,No,OOD generalization limited,Improved performance under dynamic arrivals
li2021_energyaware,Energy-Aware Job Shop Scheduling with Deep Reinforcement Learning,2021,Manufacturing scheduling,Job shop with energy cost,Simulation,DQN/PPO,State includes machine energy/profile,Job dispatching with energy-aware choices,Combined makespan/energy cost reward,Energy constraint penalties,Rule-based baselines,energy-aware heuristics,Makespan, energy cost,Energy-augmented JSS instances,No,Not reported,DRL reduces energy with minimal throughput loss
tassel2020_transferjss,A Reinforcement Learning Environment for Job-Shop Scheduling,2020,Manufacturing scheduling,Job shop,Simulation environment,Gym environment for JSS,Graph or tabular states,Dispatching action space,Flexible reward shaping,Masks for infeasible actions,Heuristics (EDD, SPT), optional OR baselines,Makespan/TWT,FT/Taillard-style instances,Yes,Train/test splits across instances,Provides standard RL env; baseline comparisons
mao2016_resource,Resource Management with DRL (DeepRM),2016,Cloud/cluster scheduling,Task scheduling in clusters,Simulation,DQN,Cluster resource state vector,Schedule next job/action bundle,Reward is negative slowdown/queueing,Constraints via simulator feasibility,Heuristics (SJF, Tetris),Makespan, average slowdown,Synthetic cluster traces,Yes,Seen/unseen job distributions,DRL beats heuristics on synthetic traces
wei2020_edge,Edge Computing Task Scheduling with DRL,2020,Edge computing,Task offloading/scheduling,Simulation,DQN/Actor-critic,Edge node load and network states,Task-to-node assignment actions,Throughput/latency/energy rewards,Constraints via capacity penalties,Greedy heuristics, optimization baselines,Latency, energy, throughput,Edge workloads,No,Not reported,DRL improves latency-energy trade-offs
farzipoor2022_cloud,Deep RL for Energy-Aware Cloud Task Scheduling,2022,Cloud computing,Cloud VM/task scheduling,Simulation,DQN/PPO,Resource utilization and SLA states,Task-to-VM mapping,Reward combines energy and SLA,Penalty for SLA violation,Heuristics, metaheuristics,Makespan, energy, SLA adherence,Cloud workload traces,No,Not detailed,Energy savings with comparable SLA
nguyen2020_transport,RL for Transport Vehicle Scheduling in Flexible Manufacturing,2020,Intralogistics/transport,AGV dispatching in FMS,Simulation,DQN,Transport network state,AGV routing/dispatching,Throughput/tardiness reward,Feasibility via simulator,Heuristics (nearest vehicle),Makespan, tardiness,FMS transport scenarios,No,Not reported,Improves throughput over heuristics
gao2020_drltransport,DRL for Transport Scheduling in Flexible Manufacturing,2020,Intralogistics/transport,Material handling scheduling,Simulation,A3C/PPO,Queue and vehicle status,Dispatching/route selection,Throughput and delay penalties,Capacity constraints masked,Heuristics, metaheuristics,Makespan, throughput,Manufacturing transport benchmarks,No,Limited,Better throughput with DRL
huang2021_semiconductor,DRL for Semiconductor Manufacturing Scheduling,2021,Semiconductor fab scheduling,Complex flow shop,Simulation,Actor-critic,Wafer lot and tool states,Tool/layer dispatching actions,Throughput and cycle time rewards,Feasibility masked/penalized,Heuristics, fab-specific rules,Cycle time, throughput,Semiconductor fab dataset,No,Not reported,DRL improves cycle time
luo2020_dynamicfjsp,Dynamic Scheduling for Flexible Job Shop with New Job Insertions by Deep Reinforcement Learning,2020,Manufacturing scheduling,Flexible job shop (dynamic with new jobs),Simulation,Deep RL (DDQN-based),Queue/machine/operation state with new arrivals,Operation sequencing with feasibility masking,Weighted makespan/tardiness,Machine availability masking,Heuristics (SPT/EDD),GA,SA,Makespan,tardiness,FT/Taillard-style dynamic instances,No,Train/test dynamic arrivals,Outperforms PDR and GA/SA on dynamic insertions
chen2020_selflearningfjsp,A Self-learning GA Based on Reinforcement Learning for Flexible Job-shop Scheduling,2020,Manufacturing scheduling,Flexible job shop,Simulation,Hybrid RL + genetic algorithm,State for GA population and shop status,Action: choose crossover/mutation strategies,Multi-objective (makespan/tardiness),Feasibility via GA repair,Classical GA/SA heuristics,Makespan,tardiness,Standard FJSP benchmarks,No,Not reported,RL-guided operator selection improves GA performance
luo2021_multiobjectivefjsp,Dynamic Multi-objective Scheduling for Flexible Job Shop by Deep Reinforcement Learning,2021,Manufacturing scheduling,Flexible job shop (dynamic),Simulation,Deep RL (actor-critic),Queue/machine status with dynamic arrivals,Dispatch/routing decisions,Multi-objective makespan+tardiness,Masking for machine feasibility,PDRs and metaheuristics,Makespan,tardiness,Dynamic FJSP benchmarks,No,Unseen instance sizes/dynamic arrivals,Improves Pareto front vs heuristics on dynamic cases
du2022_kbrl_fjsp,Knowledge-Based RL and Estimation of Distribution Algorithm for Flexible Job Shop,2022,Manufacturing scheduling,Flexible job shop (energy-aware),Simulation,Hybrid RL + EDA,State: operation/machine status incl. ToU price,Actions: operator selections and schedule refinements,Multi-objective makespan + energy/tardiness,Feasibility via encoding and repair,MILP (CPLEX), GA/EDA,Pareto: makespan, energy, tardiness,Energy-priced FJSP instances,No,Not reported,Outperforms GA/EDA and approaches MILP quality with lower cost
liu2022_drl_fjsp,Deep Reinforcement Learning for Dynamic Scheduling of a Flexible Job Shop,2022,Manufacturing scheduling,Flexible job shop (dynamic arrivals),Simulation,DDQN (hierarchical/distributed),State: job queue, machine status; hierarchical agents,Actions: dispatch next operation, schedule routing,Reward shapes makespan/tardiness,Feasibility enforced by eligibility masking,Dispatching rules, heuristic baselines,Makespan, tardiness,Dynamic FJSS instances,No,Dynamic arrival test cases,DDQN improves makespan over dispatching rules in dynamic settings
holler2019_icdm_dispatch,Deep Reinforcement Learning for Multi-driver Vehicle Dispatching and Repositioning,2019,Logistics/transport,Ride-hailing fleet dispatch (multi-driver),Simulation,Deep RL (system-centric),State: supply/demand heatmaps, driver/order status,Actions: dispatch orders, reposition drivers,Reward: revenue/profit, wait time penalties,Capacity and availability constraints in sim,Heuristics/dispatch baselines,Profit, wait time, service rate,Ride-hailing simulations,No,Unseen demand scenarios,DRL improves profit and service metrics over heuristic dispatching
liu2022_deepdispatching,Deep Dispatching: DRL for Vehicle Dispatching on Ride-Hailing Platforms,2022,Logistics/transport,Ride-hailing dispatch,Simulation,Deep RL,State: spatiotemporal supply/demand grids; driver states,Actions: assign orders/relocate drivers,Revenue/service-level reward,Capacity and service constraints,Heuristic dispatching,Profit, wait time, service rate,Ride-hailing simulations,No,Scenario variation,DRL improves profit and service level vs heuristics
varnous2024_deepdispatch_air,DeepDispatch for Advanced Air Mobility,2024,Logistics/transport,Air taxi/vertiport dispatch,Simulation,Deep RL (single + multi-agent dueling DQN),State: vertiport pads, demand, energy/pricing,Actions: dispatch/reposition air taxis,Profit and cost reward (incl. energy/charging),Pad capacity and scheduling constraints,Heuristic + optimization baselines,Operating profit, wait time, energy cost,Air taxi simulation cases,No,Multiple demand/vehicle scales,Near-optimal profit vs optimization with faster runtime
mauro2023_offline,Offline RL for Dynamic Job Shop Scheduling,2023,Manufacturing scheduling,Dynamic job shop,Offline RL,Dataset-derived states,Dispatching decisions,Weighted tardiness/makespan,Constraints via dataset feasibility,Heuristics, OR baselines,Makespan/tardiness,Historical production logs,No,Train/test splits,Offline RL competitive without simulators
shen2020_multiobjective,Multiobjective RL for Production Scheduling,2020,Manufacturing scheduling,Job shop multiobjective,Simulation,MORL (e.g., multi-objective DQN),Multi-criteria states,Dispatching with preference vectors,Vectorized rewards (makespan, tardiness, energy),Constraint penalties,Heuristics/metaheuristics,Makespan, tardiness, energy,Benchmark instances,No,Not discussed,Offers Pareto policies
zhu2022_mask,RL with Masked Action Spaces for Constrained Scheduling,2022,Constrained scheduling,Job shop/flow variants,Simulation,Masked RL (PPO),State includes constraint context,Action masking of infeasible moves,Reward for throughput/tardiness,Hard constraint masking,Heuristics, OR baselines,Makespan/tardiness,Benchmark instances,No,OOD generalization limited,Masking improves feasibility/stability
mao2019_deeprm2,Learning Scheduling Algorithms for Data Processing Clusters (DeepRM2),2019,Cloud/cluster scheduling,Task scheduling,Simulation,Policy gradient (REINFORCE),Cluster resource state,Task-slot assignment,Reward negative slowdown,Feasibility via simulator,Heuristics (SJF, Tetris),Slowdown, queue time,Synthetic traces,Yes,Generalization to new workloads,Improved slowdown over heuristics
tong2023_meta,Meta-RL for Generalizable Job Shop Scheduling,2023,Manufacturing scheduling,Job shop,Simulation,Meta-RL (e.g., MAML/PPO),State embedding across tasks,Dispatching actions,Reward blended tardiness/makespan,Constraints via masking,Heuristics baselines,Makespan/tardiness,Meta-train/test instances,No,Evaluates cross-instance generalization,Meta-RL improves adaptation
gottwald2021_safe,Safe DRL for Production Scheduling,2021,Production scheduling safety,Job/flow shop,Simulation,Safe RL (shielding/penalties),State includes safety-critical signals,Dispatching with safety checks,Rewards penalize unsafe actions,Safety shields/masking,Heuristics; OR baselines,Makespan, tardiness, safety violations,Simulation benchmarks,No,Stress tests for reliability,Safety layer reduces violations
lei2023_hrl_fjsp,Large-Scale Dynamic Scheduling for Flexible Job-Shop With Random Arrivals of New Jobs by Hierarchical Reinforcement Learning,2023,Manufacturing scheduling,Flexible job shop (dynamic arrivals),Simulation,Hierarchical DDQN/HRL,Job queues; machine load; arrival indicators,High-level selects operation; low-level assigns machine/route,Weighted makespan/tardiness with step penalties,Machine availability and precedence masks,Dispatching rules (SPT/EDD); metaheuristics,Makespan; tardiness,Dynamic FJSS benchmarks with new arrivals,No,Tests across sizes/arrival rates,HRL scales to large dynamic FJSS and cuts tardiness vs PDRs,DOI:10.1109/tii.2023.3272661
li2023_coevo_energy,Co-Evolution With Deep Reinforcement Learning for Energy-Aware Distributed Heterogeneous Flexible Job Shop Scheduling,2023,Manufacturing scheduling,Distributed heterogeneous FJSP (energy-aware),Simulation,Hybrid co-evolution + Q-learning,Operation/machine features incl. time-of-use price,Action selects evolutionary operators and schedule refinements,Reward combines makespan and total energy cost,Feasibility via encoding/repair and machine eligibility masks,GA/EDA; MILP (CPLEX),Makespan; energy; tardiness,Distributed FJSP benchmarks (20 instances),No,Not reported,Hybrid DQCE improves energy and makespan vs GA/EDA; close to MILP,DOI:10.1109/tsmc.2023.3305541
zhang2024_memetic_energyagv,Deep reinforcement learning-based memetic algorithm for energy-aware flexible job shop scheduling with multi-AGV,2024,Manufacturing scheduling + intralogistics,FJSP with integrated AGV transport (energy-aware),Simulation,RL-enhanced memetic (PPO-guided operators),Machine status; AGV positions; energy tariffs,Actions choose routing/dispatch operators for jobs and AGVs,Reward weighted makespan+energy with tardiness penalties,Feasibility via decoding and AGV collision checks,NSGA-II/IMOEA-D; dispatching heuristics,Makespan; energy consumption; transport delay,Kacem/BRdata benchmarks with multi-AGV,No,Scenario variations on job mix and AGV count,RL-guided memetic cuts energy while maintaining throughput,DOI:10.1016/j.cie.2024.109917
liu2024_gat_drl_jss,Dynamic Job-Shop Scheduling via Graph Attention Networks and Deep Reinforcement Learning,2024,Manufacturing scheduling,Dynamic job shop,Simulation,GAT encoder + actor-critic,Graph attention over operation and machine nodes,Action selects next eligible operation-machine pair,Reward negative makespan/tardiness with step penalties,Precedence and machine eligibility masks,Dispatching rules (EDD/SPT); DRL baselines,Makespan; tardiness,Taillard/Lawrence dynamic JSS benchmarks,No,Train/test splits across instance sizes,GAT-RL improves makespan vs PDRs and prior DRL on dynamic JSS,DOI:10.1109/tii.2024.3371489
rajeh2024_marltaxi,A Clustering-Based Multi-Agent Reinforcement Learning Framework for Finer-Grained Taxi Dispatching,2024,Logistics/transport,Ride-hailing/taxi dispatch,Simulation plus historical traces,Multi-agent actor-critic,Supply/demand heatmaps and cluster queues,Dispatch/reposition taxis across clusters,Revenue with wait-time and service penalties,Vehicle availability and matching constraints,Heuristic and optimization dispatchers,Revenue; wait time; service rate,Large-scale taxi trace simulations,No,Cross-day/peak scenario tests,MARL improves revenue and service rates vs heuristic dispatch,DOI:10.1109/tits.2024.3370820
drungilas2023_agv_energy,Deep Reinforcement Learning for Energy-Efficient AGV Routing in Container Terminals,2023,Logistics/transport,AGV path planning and dispatch (energy-aware),Simulation,MADDPG,AGV positions; queue lengths; battery levels,Next-node/route choices for each AGV,Energy and travel-time penalties with throughput bonus,Collision and capacity rules enforced in simulator,Shortest-path and rule-based dispatch,Makespan; energy consumption; throughput,Container terminal-style AGV scenarios,No,Scenario variations,MADDPG reduces energy while keeping throughput comparable to heuristics,DOI:10.1016/j.aej.2022.12.057
wang2022_cea_fjsp,CEA-FJSP: Carbon emission-aware flexible job-shop scheduling based on deep reinforcement learning,2022,Manufacturing scheduling,FJSP with carbon/energy cost,Simulation,Deep actor-critic,Machine status; carbon price signals,Dispatching/routing decisions,Weighted makespan and carbon-emission reward,Machine feasibility masking,GA and dispatching heuristics,Makespan; carbon emissions; energy cost,Carbon-priced FJSP benchmarks,No,Not reported,DRL lowers emissions with small makespan loss vs GA/heuristics,DOI:10.3389/fenvs.2022.1059451
jin2023_real2sim_port,Container port truck dispatching optimization using Real2Sim based deep reinforcement learning,2023,Logistics/transport,Port truck/crane dispatch,Simulation with domain randomization,PPO with sim-to-real,Crane queues; yard layout; truck positions,Assign trucks to cranes/yard slots,Turnaround and throughput reward with delay penalties,Crane/yard capacity and precedence constraints,Rule-based dispatch; optimization heuristics,Truck turnaround; throughput,Container terminal simulation informed by logs,No,Real2Sim domain-randomized scenarios,Real2Sim PPO cuts truck time vs dispatch rules in stochastic yards,DOI:10.1016/j.ejor.2023.11.038
zeng2022_yoto_jss,You Only Train Once: A highly generalizable reinforcement learning method for dynamic job shop scheduling problem,2022,Manufacturing scheduling,Dynamic job shop,Simulation,Size-generalizable RL with graph encoding,Disjunctive graph embeddings for operations/machines,Dispatch next eligible operation,Reward for makespan/tardiness reduction,Precedence and machine availability masks,PDRs; prior GNN-RL schedulers,Makespan; tardiness,Taillard FT train/test splits,Yes (TechRxiv),Cross-size generalization tests,Single trained policy transfers across unseen JSS sizes with lower makespan,DOI:10.36227/techrxiv.20324070.v1
naimi2021_qlearning_energy,A Q-Learning Rescheduling Approach to the Flexible Job Shop Problem Combining Energy and Productivity Objectives,2021,Manufacturing scheduling,FJSP rescheduling (energy/productivity),Simulation,Q-learning,Machine workload and energy price states,Swap/reschedule operations,Aggregated reward for energy and productivity,Feasibility repair for machine routing,Rule-based rescheduling; GA,Makespan; energy consumption; throughput,BRdata-style rescheduling scenarios,No,Not reported,Q-learning rescheduler cuts energy with limited productivity loss vs heuristics,DOI:10.3390/su132313016
hu2021_agv_terminal,Multi-AGV Dynamic Scheduling in an Automated Container Terminal: A Deep Reinforcement Learning Approach,2021,Logistics/transport,AGV fleet scheduling at container terminals,Simulation,MADDPG with CNN value nets,State includes AGV locations; berth tasks; queues,Actions assign tasks/routes to AGVs,Reward for throughput with delay/idle penalties,Collision avoidance and capacity constraints,Rule-based AGV dispatch,Turnaround time; queue delay; AGV utilization,Container terminal test cases,No,Scenario variations on vessel arrivals,DRL dispatcher improves quay crane productivity and reduces truck waits,DOI:10.3390/math10234575
luo2021_partialnowait_drl,Real-Time Scheduling for Dynamic Partial-No-Wait Multiobjective Flexible Job Shop by Deep Reinforcement Learning,2021,Manufacturing scheduling,Partial no-wait multiobjective FJSP,Simulation,Actor-critic (dual-critic style),Ready times; residual no-wait buffers,Dispatch next operation and machine selection,Multi-objective reward for makespan/tardiness,No-wait and buffer constraints via masking,NSGA-II; dispatching rules,Makespan; tardiness; Pareto spread,Partial-no-wait FJSP benchmarks,No,Tests on varying job counts,DRL yields better Pareto fronts than PDRs and rivals NSGA-II,DOI:10.1109/tase.2021.3104716
gong2024_agv_energy_port,Real-time AGV scheduling optimisation with deep reinforcement learning for energy-efficiency in the container terminal yard,2024,Logistics/transport,Port yard AGV dispatch (energy-aware),Simulation,Actor-critic DRL,AGV battery levels; yard queues; travel times,Dispatch/route AGVs and charging decisions,Weighted energy and turnaround reward,AGV collision and charging constraints enforced,Yard dispatch heuristics; OR baselines,Energy use; truck turnaround time,Container terminal yard scenarios,No,Stress tests under different traffic/energy prices,DRL dispatcher reduces energy and turnaround vs yard heuristics,DOI:10.1080/00207543.2024.2325583
li2022_marl_train,Train timetabling with the general learning environment and multi-agent deep reinforcement learning,2022,Logistics/transport,Train timetable/rescheduling under disturbances,Simulation,Multi-agent DRL (actor-critic),Train states; track occupancy; delay indicators,Hold/train dispatch timing adjustments,Reward penalizes delay and energy use,Headway/safety constraints enforced in simulator,Heuristic rescheduling and optimization,Makespan (delays); on-time performance; energy use,Metro corridor disturbance scenarios,No,Tests under varied disturbance patterns,MARL reduces secondary delays vs heuristic rescheduling,DOI:10.1016/j.trb.2022.02.006
zhang2020_dispatch,Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning,2020,Manufacturing scheduling,Job shop (static/dynamic),Simulation,Policy gradient with GNN encoder,"Disjunctive graph; GNN node embeddings; size-agnostic policy","Dispatch next operation on eligible machine","-makespan/-tardiness reward with step penalty","Action masking for machine eligibility; precedence via graph","PDRs (EDD/SPT/LPT), OR heuristics","Makespan; tardiness","Taillard and Lawrence JSS instance sets","Yes (open-source)","Train on small sizes, test on larger/unseen","Learned dispatching surpasses PDRs and generalizes across sizes","Verified PDF (arXiv:2010.12367)"
park2021_graphjss,Learning to Schedule Job-Shop Problems: Representation and Policy Learning Using Graph Neural Network and RL,2021,Manufacturing scheduling,Job shop,Simulation,GNN + PPO policy,"Graph with job/machine/operation nodes; GNN encodes structure","Dispatch next operation selection (PPO)","Tardiness/makespan rewards","Mask infeasible operations; precedence respected","PDRs; RL schedulers","Makespan; tardiness","FT, LA, OR-library JSS benchmarks","Yes (arXiv)","Cross-size/parameter generalization","GNN-PPO outperforms PDRs and prior RL baselines","Verified PDF"
wang2023_dualattn,FJSS via Dual Attention Network Based Reinforcement Learning,2023,Manufacturing scheduling,Flexible job shop,Simulation,Actor-critic with dual attention (GAT),"Dual attention over operations and machines","Joint routing and sequencing actions","Weighted makespan/tardiness reward","Masking for machine eligibility/availability","Traditional PDRs; SOTA DRL; OR-Tools (exact)","Makespan; tardiness","Synthetic and public FJSS benchmarks","No reported","Evaluated on larger unseen FJSS","Strong vs PDRs; approaches exact in some cases","Verified PDF (arXiv:2305.05119)"
